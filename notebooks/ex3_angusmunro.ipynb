{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3: Siamese networks and zero-shot learning\n",
    "\n",
    "## Angus Munro\n",
    "\n",
    "### Introduction\n",
    "\n",
    "A Siamese network was trained to encode plankton images into 64-dimensional vectors, i.e. as a vector space embedding. The training data comprised approximately 220000 monochromatic images from 27 classes (species). The validation and test data each comprised 2700 images (100 from each category). The code and model was based on the provided repository at https://github.com/ketil-malde/plankton-siamese; this was not modified extensively, instead, the assignment focussed mainly on the interpretation and visualisation of the resulting encoded species \"clusters\".\n",
    "\n",
    "### Model and data\n",
    "\n",
    "The model comprised an `Inception V3` base network, global average pooling, a dropout layer, followed by a dense output layer of 64 units with sigmoid activation. \n",
    "\n",
    "The raw images were resized to 299x299 resolution, and were passed to the model, which pasted them onto a 3-channel 299x299 white template, with the monochromatic pixels constituting channel 0. Thus, the two other channels remained unused on the input side but were required due to the architecture of `Inception V3`. \n",
    "\n",
    "### Training the model\n",
    "\n",
    "The model was trained using a triplet loss function, whereby three images (a random anchor, a random positive of the same class, and a random negative of a different class) were input separately to the base network, and the model trained towards a low loss, i.e. outputs  similar for the anchor and positive, and different for the anchor and negative, as quantified by the 2-norm of output vector difference.\n",
    "\n",
    "Curriculum learning was attempted, where the validation accuracy by class was used to alter the probabilities in the triplet generator toward difficult classes, but this was found to be unwieldy and of little benefit, so the approach was abandoned. \n",
    "\n",
    "Transfer learning was used with `imagenet` weights. The default code's model was fully trainable and this was found to be suboptimal at the outset of training with the untrained new model head confounding the pretrained base model. Instead, the Inception base model was frozen for the first 5 epochs to pre-train the model head, after which the base model was unlocked (at a lower learning rate) to continue training the full model. Training was conducted stepwise in steps of 5 epochs; between each step, various information was logged and the learning rate was reduced by 10%. Additionally the model was saved at each step, enabling the resumption of training for model 'nursing'. An `Adam` optimiser was used, and a learning rate of 1E-3 for the head training and 1E-5 for the full training were found to be appropriate. \n",
    "\n",
    "The standard triplet loss was observed to fall to around 0.2 after 10 cycles each of 5 epochs, where each epoch comprised 600 batches of 20 triplets (12000 triplets per epoch). \n",
    "\n",
    "The script `train.py` performed the training, using parameters contained in `config.py`, and the output of the training epochs is reproduced below (though the training was not done in Jupyter Notebook):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Epoch 1/5\n",
    "600/600 [==============================] - 639s 1s/step - loss: 1.2891 - val_loss: 0.9695\n",
    "\n",
    "Epoch 2/5\n",
    "600/600 [==============================] - 574s 956ms/step - loss: 1.0145 - val_loss: 0.8825\n",
    "\n",
    "Epoch 3/5\n",
    "600/600 [==============================] - 574s 956ms/step - loss: 0.9749 - val_loss: 0.8263\n",
    "\n",
    "Epoch 4/5\n",
    "600/600 [==============================] - 575s 958ms/step - loss: 0.8633 - val_loss: 0.7616\n",
    "\n",
    "Epoch 5/5\n",
    "600/600 [==============================] - 574s 956ms/step - loss: 0.8224 - val_loss: 0.6671\n",
    "\n",
    "Epoch 1/5\n",
    "600/600 [==============================] - 575s 958ms/step - loss: 0.7824 - val_loss: 0.7063\n",
    "\n",
    "Epoch 2/5\n",
    "600/600 [==============================] - 575s 959ms/step - loss: 0.6954 - val_loss: 0.6225\n",
    "\n",
    "Epoch 3/5\n",
    "600/600 [==============================] - 575s 959ms/step - loss: 0.7260 - val_loss: 0.5778\n",
    "\n",
    "Epoch 4/5\n",
    "600/600 [==============================] - 575s 958ms/step - loss: 0.6634 - val_loss: 0.5825\n",
    "\n",
    "Epoch 5/5\n",
    "600/600 [==============================] - 574s 957ms/step - loss: 0.6325 - val_loss: 0.4933\n",
    "\n",
    "Epoch 1/5\n",
    "600/600 [==============================] - 575s 958ms/step - loss: 0.6089 - val_loss: 0.5784\n",
    "\n",
    "Epoch 2/5\n",
    "600/600 [==============================] - 574s 957ms/step - loss: 0.5513 - val_loss: 0.4322\n",
    "\n",
    "Epoch 3/5\n",
    "600/600 [==============================] - 574s 957ms/step - loss: 0.5685 - val_loss: 0.5239\n",
    "\n",
    "Epoch 4/5\n",
    "600/600 [==============================] - 574s 956ms/step - loss: 0.5499 - val_loss: 0.4774\n",
    "\n",
    "Epoch 5/5\n",
    "600/600 [==============================] - 575s 958ms/step - loss: 0.5299 - val_loss: 0.4233\n",
    "\n",
    "Epoch 1/5\n",
    "600/600 [==============================] - 574s 957ms/step - loss: 0.5105 - val_loss: 0.3972\n",
    "\n",
    "Epoch 2/5\n",
    "600/600 [==============================] - 573s 955ms/step - loss: 0.4738 - val_loss: 0.3855\n",
    "\n",
    "Epoch 3/5\n",
    "600/600 [==============================] - 574s 957ms/step - loss: 0.4902 - val_loss: 0.4417\n",
    "\n",
    "Epoch 4/5\n",
    "600/600 [==============================] - 575s 958ms/step - loss: 0.4592 - val_loss: 0.4689\n",
    "\n",
    "Epoch 5/5\n",
    "600/600 [==============================] - 576s 960ms/step - loss: 0.4526 - val_loss: 0.5247\n",
    "\n",
    "Epoch 1/5\n",
    "600/600 [==============================] - 574s 957ms/step - loss: 0.4414 - val_loss: 0.3077\n",
    "\n",
    "Epoch 2/5\n",
    "600/600 [==============================] - 574s 957ms/step - loss: 0.4243 - val_loss: 0.3328\n",
    "\n",
    "Epoch 3/5\n",
    "600/600 [==============================] - 574s 957ms/step - loss: 0.3814 - val_loss: 0.4663\n",
    "\n",
    "Epoch 4/5\n",
    "600/600 [==============================] - 574s 957ms/step - loss: 0.4005 - val_loss: 0.4510\n",
    "\n",
    "Epoch 5/5\n",
    "600/600 [==============================] - 574s 957ms/step - loss: 0.4041 - val_loss: 0.3711\n",
    "\n",
    "Epoch 1/5\n",
    "600/600 [==============================] - 574s 957ms/step - loss: 0.3945 - val_loss: 0.3418\n",
    "\n",
    "Epoch 2/5\n",
    "600/600 [==============================] - 575s 959ms/step - loss: 0.3603 - val_loss: 0.3398\n",
    "\n",
    "Epoch 3/5\n",
    "600/600 [==============================] - 574s 957ms/step - loss: 0.3428 - val_loss: 0.3252\n",
    "\n",
    "Epoch 4/5\n",
    "600/600 [==============================] - 574s 957ms/step - loss: 0.3696 - val_loss: 0.3065\n",
    "\n",
    "Epoch 5/5\n",
    "600/600 [==============================] - 575s 958ms/step - loss: 0.3653 - val_loss: 0.3227\n",
    "\n",
    "Epoch 1/5\n",
    "600/600 [==============================] - 575s 959ms/step - loss: 0.3522 - val_loss: 0.3545\n",
    "\n",
    "Epoch 2/5\n",
    "600/600 [==============================] - 574s 957ms/step - loss: 0.3164 - val_loss: 0.3058\n",
    "\n",
    "Epoch 3/5\n",
    "600/600 [==============================] - 576s 960ms/step - loss: 0.3117 - val_loss: 0.3013\n",
    "\n",
    "Epoch 4/5\n",
    "600/600 [==============================] - 574s 957ms/step - loss: 0.3425 - val_loss: 0.2539\n",
    "\n",
    "Epoch 5/5\n",
    "600/600 [==============================] - 575s 958ms/step - loss: 0.3341 - val_loss: 0.2749\n",
    "\n",
    "Epoch 1/5\n",
    "600/600 [==============================] - 574s 956ms/step - loss: 0.3324 - val_loss: 0.3018\n",
    "\n",
    "Epoch 2/5\n",
    "600/600 [==============================] - 575s 958ms/step - loss: 0.3168 - val_loss: 0.3463\n",
    "\n",
    "Epoch 3/5\n",
    "600/600 [==============================] - 575s 958ms/step - loss: 0.3015 - val_loss: 0.2841\n",
    "\n",
    "Epoch 4/5\n",
    "600/600 [==============================] - 575s 959ms/step - loss: 0.3117 - val_loss: 0.3029\n",
    "\n",
    "Epoch 5/5\n",
    "600/600 [==============================] - 575s 958ms/step - loss: 0.3140 - val_loss: 0.2510\n",
    "\n",
    "Epoch 1/5\n",
    "600/600 [==============================] - 575s 958ms/step - loss: 0.3078 - val_loss: 0.2591\n",
    "\n",
    "Epoch 2/5\n",
    "600/600 [==============================] - 574s 956ms/step - loss: 0.2894 - val_loss: 0.2666\n",
    "\n",
    "Epoch 3/5\n",
    "600/600 [==============================] - 574s 957ms/step - loss: 0.2734 - val_loss: 0.2700\n",
    "\n",
    "Epoch 4/5\n",
    "600/600 [==============================] - 576s 959ms/step - loss: 0.2937 - val_loss: 0.3135\n",
    "\n",
    "Epoch 5/5\n",
    "600/600 [==============================] - 574s 957ms/step - loss: 0.2777 - val_loss: 0.2941\n",
    "\n",
    "Epoch 1/5\n",
    "600/600 [==============================] - 575s 959ms/step - loss: 0.2790 - val_loss: 0.2878\n",
    "\n",
    "Epoch 2/5\n",
    "600/600 [==============================] - 575s 959ms/step - loss: 0.2745 - val_loss: 0.2990\n",
    "\n",
    "Epoch 3/5\n",
    "600/600 [==============================] - 574s 957ms/step - loss: 0.2797 - val_loss: 0.2620\n",
    "\n",
    "Epoch 4/5\n",
    "600/600 [==============================] - 574s 957ms/step - loss: 0.2695 - val_loss: 0.2488\n",
    "\n",
    "Epoch 5/5\n",
    "600/600 [==============================] - 576s 960ms/step - loss: 0.2426 - val_loss: 0.2037\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A plot of the progression of training and validation loss is shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_epoch_losses()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A plot of the average cluster radius compared to the average cluster separation is shown below. (observations...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_train_radii_separation(C.obj_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the embedding\n",
    "\n",
    "The trained embedder was investigated using several means. Firstly, a confusion plot demonstrates the class for which it performs well, and those which it confuses. Secondly, PCA was applied to elucidate the components which explain the most variance in the embedding, and the embedded validation data clusters were visualised using `ployly` with a 3D plot of the leading 3 dimensions, including an animation over the course of training. \n",
    "\n",
    "Below is a confusion plot of the model's classifications, where the classification for each embedding is simply the closest class centroid:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 20 #which model to use\n",
    "cmat, classes = confusion_matrix(clusters, i-1)\n",
    "plot_confusion_matrix(cmat,\n",
    "                          classes,\n",
    "                          title='Confusion matrix for {}th iteration'.format(i),\n",
    "                          cmap=None,\n",
    "                          normalize=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WinError 3] The system cannot find the path specified: '/home/angus_munro/projects/ex3/INF368-Ex3'\n",
      "C:\\Dropbox\\UiB\\INF368_SelTopicsMachineLearning\\projects\\ex3\\INF368-Ex3\\notebooks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'create_model'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-195b2890e5c2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mcreate_model\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcreate_base_network\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0min_dim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtripletize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstd_triplet_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malt_triplet_loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mgenerators\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtriplet_generator\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtesting\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mT\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'create_model'"
     ]
    }
   ],
   "source": [
    "%cd /home/angus_munro/projects/ex3/INF368-Ex3\n",
    "\n",
    "from keras.models import load_model\n",
    "from keras.callbacks import CSVLogger\n",
    "from keras.optimizers import SGD, Adam\n",
    "\n",
    "import os\n",
    "\n",
    "from create_model import create_base_network, in_dim, tripletize, std_triplet_loss, alt_triplet_loss\n",
    "from generators import triplet_generator\n",
    "import testing as T\n",
    "\n",
    "import config as C\n",
    "\n",
    "last = C.last"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It was found that the default base model, which was fully trainable, led to validation loss remaining maxed out at 5.0 during training, at least for 50 epochs. Training loss decreased in a reasonable way. It is likely that the weights in the Inception body were thrown off by the influence of the new last layer weights. To remedy this, weights of the new layer alone were first trained:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from am_plankton import set_trainable_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = create_base_network(in_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting top 1 layers of base model trainable\n"
     ]
    }
   ],
   "source": [
    "set_trainable_layers(base_model, 1)\n",
    "model = tripletize(base_model)\n",
    "#model.compile(optimizer=SGD(lr=C.learn_rate, momentum=0.9),\n",
    "#             loss=std_triplet_loss())\n",
    "model.compile(optimizer=Adam(),\n",
    "             loss=alt_triplet_loss())\n",
    "\n",
    "def avg(x):\n",
    "    return sum(x)/len(x)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
